# -*- coding: utf-8 -*-
"""HACKA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BuB-uVeKr_Ph9u4fFDUxc89s6Rial37W

# Colunas do dataset de teste:

 - semana (número inteiro): número da semana (1 a 4 de janeiro/2023)
 - pdv (número inteiro): código do ponto de venda
 - produto (número inteiro): código do SKU
 - quantidade (número inteiro): previsão de vendas
"""

!pip install parquet -q
!pip install pyarrow -q
!pip -q install duckdb -q

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import duckdb
import os
import pyarrow.parquet as pq
import pyarrow as pa # Import pyarrow core library

pd.set_option('display.max_columns', 60)
pd.options.mode.chained_assignment = None

import matplotlib.pyplot as plt
# %matplotlib inline

plt.rcParams['font.size'] = 24

from IPython.core.pylabtools import figsize

import seaborn as sns
sns.set(font_scale = 2)

from sklearn.model_selection import train_test_split

from sklearn.impute import SimpleImputer # Changed Imputer to SimpleImputer and imported from sklearn.impute
from sklearn.preprocessing import MinMaxScaler

from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer, mean_absolute_percentage_error

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import make_scorer
import pandas as pd
import numpy as np

import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
import numpy as np

"""# Modelo deve prever a quantidade com estas colunas no dataset df_teste, gerando a coluna quantidade"""

data = {'semana': [1,2,3, 4, 5],
        'pdv': [1023, 1045, 1023, 1088, 1010],
        'produto': [123,234, 456, 123, 550]}
df_teste = pd.DataFrame(data)

def missing_values_table(df):
        mis_val = df.isnull().sum()

        mis_val_percent = 100 * df.isnull().sum() / len(df)

        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)

        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values'})

        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)

        print ("Seu dataframe tem " + str(df.shape[1]) + " colunas.\n"
            "Há " + str(mis_val_table_ren_columns.shape[0]) +
              " colunas que possuem valores ausentes.")

        return mis_val_table_ren_columns

path='/content/drive/MyDrive/HACKA/'

"""# Ler os arquivos parquet com  pyarrow
- cada parte do arquivo parquet foi lida individualmente
- depois foram concatenadas
- a quantidade de linhas e colunas ficou igual a leitura com duckdb
"""

# Get the directory of the current file
directory = os.path.dirname(path)

# List all parquet files in the directory that are part of the dataset
all_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.startswith('part-') and f.endswith('.parquet')]

#print("Arquivos Parquet encontrados:")
#for file in all_files:
    #print(file)
print(f'Lendo e concatenando registros...')

tables = []
for file in all_files:
    try:
        # Read each parquet file into a PyArrow table
        table = pq.read_table(file)
        tables.append(table)
        print("\nTables Pyarrow:\n", tables)
    except Exception as e:
        print(f"Error reading file {file}: {e}")

if tables:
    # Concatenate the PyArrow tables
    # Use the promote parameter to handle schema differences by promoting to a common type
    combined_table = pa.concat_tables(tables, promote=True)

    # Convert the combined PyArrow Table to a Pandas DataFrame
    df_pyarrow_combined = combined_table.to_pandas()

    print("\nDataset pandas lido e concatenado pelo PyArrow (todas as colunas):")
    print("Colunas:", df_pyarrow_combined.columns.tolist())
    print(df_pyarrow_combined.shape)
else:
    print("No tables were read.")

df_pyarrow_combined.columns

df_pyarrow_combined.head()

"""# Leitura dos arquivos parquet com duckdb
- esta leitura foi descontinuada pois o pyarrow ja fez isso.
"""

"""
# (opcional) limite de memória e nº de threads
con = duckdb.connect()
con.execute("PRAGMA memory_limit='6GB';")   # ajuste conforme sua RAM
con.execute("PRAGMA threads=4;")             # ajuste conforme CPU

parquet_path = '/content/drive/MyDrive/HACKA/'
out_path     = "/content/sample_1M.parquet"                    # saída
number_of_lines = 8000000 # quantidade de linhas a serem lidas

# Get the directory of the current file
directory = os.path.dirname(parquet_path)

# List all parquet files in the directory that are part of the dataset
all_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.startswith('part-') and f.endswith('.parquet')]

print("Arquivos Parquet encontrados:")
for file in all_files:
    print(file)
print(f'Lendo {number_of_lines} de registros...')

# DICA: se você só precisa de algumas colunas, selecione-as para reduzir IO/RAM
cols = "*"  # ex.: "col1, col2, col3"

# Amostra uniforme exata de 7.000.000 lines, in streaming, com semente fixa (reprodutível)
# Using a glob pattern to read all parquet files in the directory and union by name
query = f"""
#COPY (
#  SELECT {cols}
#  FROM read_parquet('{parquet_path}*.parquet', union_by_name=True)
#  USING SAMPLE {number_of_lines} (reservoir, 42)
#) TO '{out_path}' (FORMAT PARQUET);
"""
con.execute(query)

# Read the sample into pandas/polars afterwards (now it's small):
import pandas as pd
df = pd.read_parquet(out_path)
print("\nDataset lido pelo DuckDB (todas as colunas):")
print("Colunas:", df.columns.tolist())
print(df.shape)
"""

#df.columns.tolist()
"""
['pdv',
 'premise',
 'categoria_pdv',
 'zipcode',
 'internal_store_id',
 'internal_product_id',
 'distributor_id',
 'transaction_date',
 'reference_date',
 'quantity',
 'gross_value',
 'net_value',
 'gross_profit',
 'discount',
 'taxes',
 'produto',
 'categoria',
 'descricao',
 'tipos',
 'label',
 'subcategoria',
 'marca',
 'fabricante']
 """

#print("PyArrow DataFrame columns:")
#print("Shape?", df_pyarrow_combined.shape)
#print(df_pyarrow_combined.columns.tolist())

#print("\nDuckDB DataFrame columns:")
#print("Shape?", df.shape)
#print(df.columns.tolist())
"""
PyArrow DataFrame columns:
Shape? (6582209, 23)
['internal_store_id', 'internal_product_id', 'distributor_id', 'transaction_date', 'reference_date', 'quantity', 'gross_value', 'net_value', 'gross_profit', 'discount', 'taxes', 'produto', 'categoria', 'descricao', 'tipos', 'label', 'subcategoria', 'marca', 'fabricante', 'pdv', 'premise', 'categoria_pdv', 'zipcode']

DuckDB DataFrame columns:
Shape? (6582209, 23)
['pdv', 'premise', 'categoria_pdv', 'zipcode', 'internal_store_id', 'internal_product_id', 'distributor_id', 'transaction_date', 'reference_date', 'quantity', 'gross_value', 'net_value', 'gross_profit', 'discount', 'taxes', 'produto', 'categoria', 'descricao', 'tipos', 'label', 'subcategoria', 'marca', 'fabricante']
"""

"""# Coluna e tipos esperados no dataset de teste e submissão
- coluna semana no ano (1 a 52)
- pdv (número inteiro): código do ponto de venda
- produto (número inteiro): código do SKU
- quantidade (número inteiro): previsão de vendas
"""

df = df_pyarrow_combined.copy()

"""# Estatistica Descritiva"""

df.describe().T

"""# Tipo de cada coluna"""

df.info()

"""# Percentual de dados ausentes por coluna"""

missing_values_table(df)

"""# Colunas pdv e produto apresentam 99% de dados ausentes.
# Serao substituidas por
- internal_store_id
- internal_product_id
"""

colunas = ['internal_store_id',  'pdv',  'internal_product_id', 'produto', 'transaction_date','quantity']

missing_values_table(df[colunas])

"""# Ano  remover as linhas ausentes em quantity as linhas de outras colunas também foram removidas."""

df = df.dropna(subset=["quantity"]).copy()
missing_values_table(df[colunas])

df.columns

df2 = df[['internal_store_id', 'internal_product_id', 'transaction_date', 'quantity']].copy()

"""# Coluna são renomeadas"""

df2.rename(columns={'internal_store_id': 'pdv', 'internal_product_id': 'produto', 'quantity': 'quantidade'}, inplace=True)

df2.isna().sum()

"""# Extrair o numero da semana no ano a partir da data de transacao"""

df2['transaction_date'] = pd.to_datetime(df2['transaction_date'])
df2['semana'] = df2['transaction_date'].dt.isocalendar().week
del df2['transaction_date']

df2.columns

df2

missing_values_table(df2)

df2.info()

df2

df2.nunique()

"""# Coluna quantidade tem valores negativos
# Outlier certamente.
"""

df2.describe()

"""# Extrair apenas 10% dos dados"""

df_baseline = df2.sample(frac=0.1, random_state=42)
df_baseline.shape

"""# Baseline para referência"""

X = df_baseline.drop('quantidade', axis=1)
y = df_baseline['quantidade']
#
print(X.shape)
print(y.shape)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

X_train.isna().sum()

X_test.isna().sum()

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

# ---------------------------
#  Pipeline

# Define categorical and numerical features
categorical_features = ['pdv', 'produto']
numerical_features = ['semana']

# Create a column transformer to apply different preprocessing steps to different columns
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
        ('num', StandardScaler(), numerical_features)
    ],
    remainder='passthrough' # Keep other columns (if any) - not strictly necessary here
)


pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ("model", LinearRegression())
])

# ---------------------------
# 4. Avaliação com MAPE (cross-validation)
# ---------------------------
def wmape(y_true, y_pred):
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))

# Definir scorer com MAPE (quanto menor, melhor)
#mape_scorer = make_scorer(wmape, greater_is_better=False)

#scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring=mape_scorer)

#print("WMAPE médio (cross-val):", -scores.mean())
#print("WMAPE por fold:", -scores)

"""
Resultado usando LinearRegression e 10% dos dados:
WMAPE médio (cross-val): 1.2183947001894604
WMAPE por fold: [1.20463883 1.21263043 1.23791484]
"""

# ---------------------------
# 5. Treinar modelo final e prever no teste
# ---------------------------
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
print("WMAPE test:", wmape(y_test, y_pred))
#
# Novo dataset para submissao
# Em df_teste colunas pdv e  produto devem ser object.
df_teste['pdv'] = df_teste['pdv'].astype(str)
df_teste['produto'] = df_teste['produto'].astype(str)

yp_pred = pipeline.predict(df_teste)
df_teste['quantidade'] = yp_pred.astype(int)
df_teste.head()



"""# Remover Outliers"""

df_com_outliers = df2.copy()
df_com_outliers.shape

df_com_outliers.head(3)

df_com_outliers.describe()

"""# Modo padrão usando IQT,  Q1 e Q3"""

first_quartile = df_com_outliers['quantidade'].describe()['25%']
third_quartile = df_com_outliers['quantidade'].describe()['75%']

iqr = third_quartile - first_quartile

df_sem_outliers= df_com_outliers[(df_com_outliers['quantidade'] > (first_quartile - 1.5 * iqr)) &
            (df_com_outliers['quantidade'] < (third_quartile + 1.5 * iqr))].copy()

def remove_outliers(df):
    print("Antes:", df.shape)
    first_quartile = df_com_outliers['quantidade'].describe()['25%']
    third_quartile = df_com_outliers['quantidade'].describe()['75%']

    iqr = third_quartile - first_quartile

    df_sem_outliers= df_com_outliers[(df_com_outliers['quantidade'] > (first_quartile - 1.5 * iqr)) &
            (df_com_outliers['quantidade'] < (third_quartile + 1.5 * iqr))].copy()
    df_sem_outliers = df_sem_outliers[df_sem_outliers['quantidade'] >= 0]
    print("Depois:", df_sem_outliers.shape)
    return df_sem_outliers

df_sem_outliers = remove_outliers(df_com_outliers)

df_sem_outliers.describe()

df_sem_outliers.info()

"""# Divisão de treino e teste a ser usado no treinamento e avaliação"""

X = df_sem_outliers.drop('quantidade', axis=1)
y = df_sem_outliers['quantidade']
#
print(X.shape)
print(y.shape)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

"""# Treinamento e avaliação WMAPE sem outliers

# LinearRegression
"""

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

# ---------------------------
#  Pipeline

# Define categorical and numerical features
categorical_features = ['pdv', 'produto']
numerical_features = ['semana']

# Pipelines para cada tipo
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

numerical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("scaler", StandardScaler())
])

# Create a column transformer to apply different preprocessing steps to different columns
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', categorical_transformer, categorical_features),
        ('num', numerical_transformer, numerical_features)
    ]
)

pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ("model", LinearRegression())
])

# ---------------------------
# 4. Avaliação com MAPE (cross-validation)
# ---------------------------

def wmape(y_true, y_pred):
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))

# Definir scorer com WMAPE (quanto menor, melhor)
#mape_scorer = make_scorer(wmape, greater_is_better=False)

#scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring=mape_scorer)

#print("WMAPE médio (cross-val):", -scores.mean())
#print("WMAPE por fold:", -scores)

"""
Resultado usando LinearRegression e 10% dos dados:
WMAPE médio (cross-val): 0.4456727354526528
WMAPE por fold: [0.44531342 0.44669343 0.44501136]
"""

# ---------------------------
# 5. Treinar modelo final e prever no teste
# ---------------------------
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
print("WMAPE test:", wmape(y_test, y_pred))
#
# Novo dataset para submissao
# Em df_teste colunas pdv e  produto devem ser object.
df_teste['pdv'] = df_teste['pdv'].astype(str)
df_teste['produto'] = df_teste['produto'].astype(str)
yp_pred = pipeline.predict(df_teste)
df_teste['quantidade'] = yp_pred.astype(int)
df_teste.head()

"""# RandomForestRegressor"""

import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import make_scorer

# -----------------------------
# Métrica WMAPE
# -----------------------------
def wmape(y_true, y_pred):
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))

wmape_scorer = make_scorer(wmape, greater_is_better=False)  # menor = melhor

# -----------------------------
# Feature Engineer
# -----------------------------
class StatsFeatureEngineer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.stats_pdv_mean = None
        self.stats_prod_mean = None
        self.stats_semana_mean = None
        self.stats_pdv_median = None
        self.stats_prod_median = None
        self.stats_semana_median = None

    def fit(self, X, y=None):
        df = X.copy()
        df["quantidade"] = y

        # Médias
        self.stats_pdv_mean = df.groupby("pdv")["quantidade"].mean().rename("pdv_mean_qtd")
        self.stats_prod_mean = df.groupby("produto")["quantidade"].mean().rename("prod_mean_qtd")
        self.stats_semana_mean = df.groupby("semana")["quantidade"].mean().rename("semana_mean_qtd")

        # Medianas
        self.stats_pdv_median = df.groupby("pdv")["quantidade"].median().rename("pdv_median_qtd")
        self.stats_prod_median = df.groupby("produto")["quantidade"].median().rename("prod_median_qtd")
        self.stats_semana_median = df.groupby("semana")["quantidade"].median().rename("semana_median_qtd")

        return self

    def transform(self, X):
        df = X.copy()

        df = df.join(self.stats_pdv_mean, on="pdv")
        df = df.join(self.stats_prod_mean, on="produto")
        df = df.join(self.stats_semana_mean, on="semana")

        df = df.join(self.stats_pdv_median, on="pdv")
        df = df.join(self.stats_prod_median, on="produto")
        df = df.join(self.stats_semana_median, on="semana")

        return df.fillna(0)

# -----------------------------
# Pipeline
# -----------------------------
categorical_features = ["pdv", "produto"]
numerical_features = ["semana", "pdv_mean_qtd", "prod_mean_qtd", "semana_mean_qtd",
                      "pdv_median_qtd", "prod_median_qtd", "semana_median_qtd"]

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

numerical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("scaler", StandardScaler())
])

col_transformer = ColumnTransformer(
    transformers=[
        ("cat", categorical_transformer, categorical_features),
        ("num", numerical_transformer, numerical_features)
    ]
)


pipeline = Pipeline(steps=[
    ("feature_engineer", StatsFeatureEngineer()),
    ("preprocessor", col_transformer),
    ("regressor", RandomForestRegressor(random_state=42))
])

# -----------------------------
# Cross-validation simples
# -----------------------------
scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring=wmape_scorer)
print("WMAPE médio (cross-val):", -scores.mean())
print("WMAPE por fold:", -scores)

# -----------------------------
# GridSearchCV
# -----------------------------
param_grid = {
    "regressor__n_estimators": [100, 200],
    "regressor__max_depth": [10, 20, None],
    "regressor__min_samples_split": [2, 5]
}

grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=3,
    scoring=wmape_scorer,
    n_jobs=-1,
    verbose=2
)

grid_search.fit(X_train, y_train)

print("Melhores hiperparâmetros:", grid_search.best_params_)
print("Melhor WMAPE (cv):", -grid_search.best_score_)

# -----------------------------
# Avaliação final no holdout
# -----------------------------
best_pipeline = grid_search.best_estimator_
y_pred = best_pipeline.predict(X_test)
print("WMAPE test:", wmape(y_test, y_pred))

# -----------------------------
# Submissão
# -----------------------------
# Em df_teste colunas pdv e  produto devem ser object.
df_teste['pdv'] = df_teste['pdv'].astype(str)
df_teste['produto'] = df_teste['produto'].astype(str)

y_submit = best_pipeline.predict(df_teste)
df_submit = df_teste[["semana", "pdv", "produto"]].copy()
df_submit["quantidade"] = np.round(y_submit).astype(int)
df_submit.to_csv("submission.csv", sep=";", index=False, encoding="utf-8")
print("Arquivo submission.csv gerado!")

"""# Pipeline final"""

import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import make_scorer

# -----------------------------
# Métrica WMAPE
# -----------------------------
def wmape(y_true, y_pred):
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))

wmape_scorer = make_scorer(wmape, greater_is_better=False)

# -----------------------------
# Feature Engineer
# -----------------------------
class StatsFeatureEngineer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        df = X.copy()
        df["quantidade"] = y

        # Médias
        self.stats_pdv_mean = df.groupby("pdv")["quantidade"].mean().rename("pdv_mean_qtd")
        self.stats_prod_mean = df.groupby("produto")["quantidade"].mean().rename("prod_mean_qtd")
        self.stats_semana_mean = df.groupby("semana")["quantidade"].mean().rename("semana_mean_qtd")

        # Medianas
        self.stats_pdv_median = df.groupby("pdv")["quantidade"].median().rename("pdv_median_qtd")
        self.stats_prod_median = df.groupby("produto")["quantidade"].median().rename("prod_median_qtd")
        self.stats_semana_median = df.groupby("semana")["quantidade"].median().rename("semana_median_qtd")

        return self

    def transform(self, X):
        df = X.copy()
        df = df.join(self.stats_pdv_mean, on="pdv")
        df = df.join(self.stats_prod_mean, on="produto")
        df = df.join(self.stats_semana_mean, on="semana")
        df = df.join(self.stats_pdv_median, on="pdv")
        df = df.join(self.stats_prod_median, on="produto")
        df = df.join(self.stats_semana_median, on="semana")
        return df.fillna(0)

# -----------------------------
# Pipeline
# -----------------------------
# Colunas
categorical_features = ["pdv", "produto"]
numerical_features = ["semana", "pdv_mean_qtd", "prod_mean_qtd", "semana_mean_qtd",
                      "pdv_median_qtd", "prod_median_qtd", "semana_median_qtd"]

# Transformadores
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

numerical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("scaler", StandardScaler())
])

col_transformer = ColumnTransformer(
    transformers=[
        ("cat", categorical_transformer, categorical_features),
        ("num", numerical_transformer, numerical_features)
    ]
)

# Pipeline completo
pipeline = Pipeline(steps=[
    ("feature_engineer", StatsFeatureEngineer()),
    ("preprocessor", col_transformer),
    ("regressor", RandomForestRegressor(random_state=42))
])

# -----------------------------
# Cross-validation
# -----------------------------
scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring=wmape_scorer)
print("WMAPE médio (cross-val):", -scores.mean())
print("WMAPE por fold:", -scores)

# -----------------------------
# GridSearchCV
# -----------------------------
param_grid = {
    "regressor__n_estimators": [100, 200],
    "regressor__max_depth": [10, 20, None],
    "regressor__min_samples_split": [2, 5]
}

grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=3,
    scoring=wmape_scorer,
    n_jobs=-1,
    verbose=2
)

grid_search.fit(X_train, y_train)
print("Melhores hiperparâmetros:", grid_search.best_params_)
print("Melhor WMAPE (cv):", -grid_search.best_score_)

# -----------------------------
# Avaliação final
# -----------------------------
best_pipeline = grid_search.best_estimator_
y_pred = best_pipeline.predict(X_test)
print("WMAPE test:", wmape(y_test, y_pred))

# -----------------------------
# Submissão
# -----------------------------
# Garante que pdv e produto do dataset de teste sejam strings para o pipeline
df_teste['pdv'] = df_teste['pdv'].astype(str)
df_teste['produto'] = df_teste['produto'].astype(str)

# Predição
y_submit = best_pipeline.predict(df_teste)

# Monta dataframe de submissão
df_submit = df_teste[["semana", "pdv", "produto"]].copy()
df_submit["quantidade"] = np.round(y_submit).astype(int)

# Converte todas colunas para inteiro antes de salvar
df_submit["semana"] = df_submit["semana"].astype(int)
df_submit["pdv"] = df_submit["pdv"].astype(int)
df_submit["produto"] = df_submit["produto"].astype(int)

# Salva CSV final para submissão
df_submit.to_csv("submission.csv", sep=";", index=False, encoding="utf-8")
print("Arquivo submission.csv gerado!")

# ---------------------------
#  Pipeline

# Define categorical and numerical features
categorical_features = ['pdv', 'produto']
numerical_features = ['semana']

# Pipelines para cada tipo
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

numerical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("scaler", StandardScaler())
])

# Create a column transformer to apply different preprocessing steps to different columns
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', categorical_transformer, categorical_features),
        ('num', numerical_transformer, numerical_features)
    ]
)

pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ("model", RandomForestRegressor())
])

# ---------------------------
# 4. Avaliação com MAPE (cross-validation)
# ---------------------------

def wmape(y_true, y_pred):
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))

# Definir scorer com WMAPE (quanto menor, melhor)
#mape_scorer = make_scorer(wmape, greater_is_better=False)

#scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring=mape_scorer)

#print("WMAPE médio (cross-val):", -scores.mean())
#print("WMAPE por fold:", -scores)

"""
Resultado usando LinearRegression e 10% dos dados:
WMAPE médio (cross-val): 0.4456727354526528
WMAPE por fold: [0.44531342 0.44669343 0.44501136]
"""

# ---------------------------
# 5. Treinar modelo final e prever no teste
# ---------------------------
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
print("WMAPE test:", wmape(y_test, y_pred))
#
# Novo dataset para submissao
yp_pred = pipeline.predict(df_teste)
df_teste['quantidade'] = yp_pred.astype(int)
df_teste.head()

"""# Treinamento e avaliação com novas features criadas"""

import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

def wmape(y_true, y_pred):
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))

# -----------------------------
# Preprocessor customizado
# -----------------------------
class Preprocessor(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.stats_pdv = None
        self.stats_prod = None
        self.stats_semana = None

    def fit(self, X, y=None):
        # Calcula estatísticas no treino
        df = X.copy()
        df["quantidade"] = y

        # mean()
        self.stats_pdv_mean = df.groupby("pdv")["quantidade"].mean().reset_index().rename(columns={"quantidade": "pdv_mean_qtd"})
        self.stats_prod_mean = df.groupby("produto")["quantidade"].mean().reset_index().rename(columns={"quantidade": "prod_mean_qtd"})
        self.stats_semana_mean = df.groupby("semana")["quantidade"].mean().reset_index().rename(columns={"quantidade": "semana_mean_qtd"})

        # median()
        self.stats_pdv_median = df.groupby("pdv")["quantidade"].median().reset_index().rename(columns={"quantidade": "pdv_median_qtd"})
        self.stats_prod_median = df.groupby("produto")["quantidade"].median().reset_index().rename(columns={"quantidade": "prod_median_qtd"})
        self.stats_semana_median = df.groupby("semana")["quantidade"].median().reset_index().rename(columns={"quantidade": "semana_median_qtd"})

        return self

    def transform(self, X):
        df = X.copy()

        # Merge estatísticas
        df = df.merge(self.stats_pdv_mean, on="pdv", how="left")
        df = df.merge(self.stats_prod_mean, on="produto", how="left")
        df = df.merge(self.stats_semana_mean, on="semana", how="left")

        df = df.merge(self.stats_pdv_median, on="pdv", how="left")
        df = df.merge(self.stats_prod_median, on="produto", how="left")
        df = df.merge(self.stats_semana_median, on="semana", how="left")

        # Substitui NaN
        df = df.fillna(0)

        return df


# Cria pipeline de pré-processamento
preprocessor = Preprocessor()
X_train_transformed = preprocessor.fit_transform(X_train, y_train)
X_test_transformed = preprocessor.transform(X_test)

print("Colunas após o Preprocessor:")
print(X_train_transformed.columns.tolist())

# Define features automaticamente
categorical_features = ["pdv", "produto"]
numerical_features = [col for col in X_train_transformed.columns if col not in categorical_features]

# Pipelines para cada tipo
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

numerical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("scaler", StandardScaler())
])

# Create a column transformer to apply different preprocessing steps to different columns
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', categorical_transformer, categorical_features),
        ('num', numerical_transformer, numerical_features)
    ]
)

# ColumnTransformer
#column_transformer = ColumnTransformer(
#    transformers=[
#        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
#        ("num", StandardScaler(), numerical_features)
#    ]
#)

# Modelo final em pipeline
pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("regressor", RandomForestRegressor(random_state=42))
])

# Treino
pipeline.fit(X_train_transformed, y_train)

# Avaliação
# Definir scorer com WMAPE (quanto menor, melhor)
#mape_scorer = make_scorer(wmape, greater_is_better=False)

#scores = cross_val_score(pipeline, X_train_transformed, y_train, cv=3, scoring=mape_scorer)

#print("WMAPE médio (cross-val):", -scores.mean())
#print("WMAPE por fold:", -scores)

# 5. Treinar modelo final e prever no teste
# ---------------------------
# Predição
y_pred = pipeline.predict(X_test_transformed)
y_pred = pipeline.predict(X_test)
print("WMAPE test:", wmape(y_test, y_pred))
#
# Novo dataset para submissao
yp_pred = pipeline.predict(df_teste)
df_teste['quantidade'] = yp_pred.astype(int)
df_teste.head()



"""# Treinamento e avaliação usando RandomForestRegressor
# Coluna com a media de venda por pdv criada.
"""

# ---------------------------
# 2. Transformer customizado
# ---------------------------
class MediaVendasPDV(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        self.media_por_pdv_ = pd.DataFrame({"pdv": X["pdv"], "Quantidade": y}).groupby("pdv")["Quantidade"].mean()
        self.media_global_ = self.media_por_pdv_.mean()
        return self

    def transform(self, X):
        X = X.copy()
        X["Media_Vendas_PDV"] = X["pdv"].map(self.media_por_pdv_)
        X["Media_Vendas_PDV"].fillna(self.media_global_, inplace=True)
        print("Dataset gerado:", X.columns.tolist())
        return X

# ---------------------------
# 3. Pipeline
# ---------------------------
categoricas = ['pdv', "produto"]
numericas = ["semana", "Media_Vendas_PDV"] # Added 'semana' and 'Media_Vendas_PDV' to numerical features

preprocessor = ColumnTransformer([
    ("cat", OneHotEncoder(handle_unknown="ignore"), categoricas),
    ("num", StandardScaler(), numericas)
])

pipeline = Pipeline([
    ("media_pdv", MediaVendasPDV()),
    ("preprocessor", preprocessor),
    ("model", RandomForestRegressor())
])

def wmape(y_true, y_pred):
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))

# =======================================
# 4. Treinamento
# =======================================
modelo = RandomForestRegressor(
    n_estimators=200, random_state=42, n_jobs=-1
)
modelo.fit(X_train, y_train)

# =======================================
# 5. Avaliação
# =======================================
y_pred = modelo.predict(X_test)
print("WMAPE test:", wmape(y_test, y_pred))
"""
print("Processando cross_val_score...")
# Definir scorer com MAPE (quanto menor, melhor)
mape_scorer = make_scorer(wmape, greater_is_better=False)

scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring=mape_scorer)

print("WMAPE médio (cross-val):", -scores.mean())
print("WMAPE por fold:", -scores)
"""
"""
Resultado usando RandomForestRegressor e 10% dos dados:
WMAPE médio (cross-val): 0.4045400915671511
WMAPE por fold: [0.40469029 0.40565603 0.40327395]
"""
# Novo dataset para submissao
yp_pred = pipeline.predict(df_teste)
df_teste['quantidade'] = yp_pred.astype(int)
df_teste.head()





# =======================================
# 1. Função de WMAPE
# =======================================
def wmape(y_true, y_pred):
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))

# =======================================
# 2. Custom Preprocessing Transformer
# =======================================
from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline


class Preprocessor(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.pdv_mode = None
        self.produto_mode = None
        self.quantity_median = None
        self.train_stats = None  # To store stats for feature engineering
        self.train_semana_stats = None  # To store week stats from training

    def fit(self, X, y=None):
        # Ensure X is a DataFrame
        X = pd.DataFrame(X)

        # Store modes and median from the training data
        self.pdv_mode = X['pdv'].mode().iloc[0]
        self.produto_mode = X['produto'].mode().iloc[0]
        if 'quantidade' in X.columns:
            self.quantity_median = X['quantidade'].median()

        # Calculate and store train stats for feature engineering
        if 'quantidade' in X.columns and 'semana' in X.columns:
            self.train_stats = self._calculate_train_stats(X)
            # Specifically store week stats as 'semana' is in the test set
            if 'semana' in X.columns and pd.api.types.is_numeric_dtype(X['semana']):
                self.train_semana_stats = X.groupby("semana")["quantidade"].agg(
                    ["mean", "median", "std"]
                ).reset_index().rename(columns={
                    "mean": "semana_mean_qtd",
                    "median": "semana_median_qtd",
                    "std": "semana_std_qtd"
                })
            else:
                self.train_semana_stats = pd.DataFrame()  # Empty if cannot calculate

        return self

    def transform(self, X):
        # Ensure X is a DataFrame
        X = pd.DataFrame(X)

        # Apply the preprocessing steps
        X['pdv'] = X['pdv'].fillna(self.pdv_mode)
        X['produto'] = X['produto'].fillna(self.produto_mode)
        if 'quantidade' in X.columns:
            X['quantidade'] = X['quantidade'].fillna(self.quantity_median)

        # Handle 'semana' column
        if 'transaction_date' in X.columns:
            # Fill missing transaction_date before converting to datetime
            # Using mode from original training data if available, otherwise find mode in current X
            if self.train_stats and 'transaction_date_most_freq' in self.train_stats[2].columns:  # This check is incorrect, train_stats doesn't have transaction_date_most_freq
                # Need a way to get the mode of the original transaction_date if needed
                pass  # Skip this part for now, rely on filling NaT after conversion

            X['transaction_date'] = pd.to_datetime(X['transaction_date'], errors='coerce')
            # Handle potential NaT values after conversion
            if not X['transaction_date'].isnull().all():
                X['transaction_date'] = X['transaction_date'].fillna(X['transaction_date'].mode().iloc[0])
            else:
                # If all dates became NaT, fill with a dummy date
                X['transaction_date'] = X['transaction_date'].fillna(pd.to_datetime('2023-01-01'))

            X['semana'] = X['transaction_date'].dt.isocalendar().week.astype('int64')
            # Drop the original 'transaction_date'
            X = X.drop(columns=['transaction_date'])

        elif 'semana' in X.columns:
            # If 'semana' is already present, ensure it's int and use it directly
            X['semana'] = X['semana'].astype('int64')
        else:
            print("Warning: Neither 'transaction_date' nor 'semana' found. Cannot create/use 'semana' feature.")
            # Add 'semana' column with a default value or handle as needed
            X['semana'] = 0  # Add a default 'semana' to avoid errors later, adjust as needed


        # Apply feature engineering based on stored train_stats
        if self.train_stats:
            # Ensure semana stats are used if available
            self.train_stats = (self.train_stats[0], self.train_stats[1], self.train_semana_stats)  # Update with stored week stats
            X = self._apply_feature_engineering(X, self.train_stats)
        else:
            print("Warning: train_stats not available for feature engineering.")


        return X

    def _calculate_train_stats(self, df):
        # Agregações por pdv
        stats_pdv = df.groupby("pdv")["quantidade"].agg(
            ["mean", "median", "std"]
        ).reset_index().rename(columns={
            "mean": "pdv_mean_qtd",
            "median": "pdv_median_qtd",
            "std": "pdv_std_qtd"
        })

        # Agregações por produto
        stats_prod = df.groupby("produto")["quantidade"].agg(
            ["mean", "median", "std"]
        ).reset_index().rename(columns={
            "mean": "prod_mean_qtd",
            "median": "prod_median_qtd",
            "std": "prod_std_qtd"
        })

        # Return stats excluding week stats, which are handled separately
        return (stats_pdv, stats_prod, pd.DataFrame())  # Return empty DataFrame for week stats here


    def _apply_feature_engineering(self, df, train_stats):
        stats_pdv, stats_prod, stats_semana = train_stats

        # Merge features
        df = df.merge(stats_pdv, on="pdv", how="left")
        df = df.merge(stats_prod, on="produto", how="left")
        # Merge on 'semana' only if stats_semana is not empty
        if not stats_semana.empty and 'semana' in df.columns:  # Check if 'semana' is in df before merging
            df = df.merge(stats_semana, on="semana", how="left")
        elif 'semana' in df.columns:
            print("Warning: Skipping merge with week stats as it was not calculated or is empty.")


        # Fill NaNs introduced by merging (for new pdv/produto/semana in test)
        # Fill with 0 or a suitable placeholder
        df = df.fillna(0)  # Using 0 as a simple fill value

        return df


# =======================================
# 3. Pipeline
# =======================================
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import make_scorer
import pandas as pd
import numpy as np


# Define features that will be present after Preprocessor and before OneHotEncoder/StandardScaler
# These should include the original features and the new features created by the Preprocessor
features_after_preprocessing = ['pdv', 'produto', 'semana', 'pdv_mean_qtd', 'pdv_median_qtd',
                                'pdv_std_qtd', 'prod_mean_qtd', 'prod_median_qtd', 'prod_std_qtd',
                                'semana_mean_qtd', 'semana_median_qtd', 'semana_std_qtd']


# Define categorical and numerical features for the ColumnTransformer
categorical_features_for_ct = ['pdv', 'produto']
numerical_features_for_ct = [col for col in features_after_preprocessing if col not in categorical_features_for_ct]


# Create a column transformer to apply different preprocessing steps to different columns
preprocessor_ct = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features_for_ct),
        ('num', StandardScaler(), numerical_features_for_ct)
    ],
    remainder='passthrough' # Keep other columns (if any)
)

pipeline = Pipeline([
    ('custom_preprocessor', Preprocessor()), # Add the custom preprocessor
    ('column_transformer', preprocessor_ct), # Add the column transformer
    ("model", RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)) # Add the model
])


# =======================================
# Prepare data for the pipeline
# =======================================
# The pipeline expects the raw data with columns needed by the Preprocessor
# We will use the df_sem_outliers as the input to the pipeline for training
# Include 'transaction_date' and 'quantity' as they are needed by the Preprocessor
Xy_train_raw = df_sem_outliers[['pdv', 'produto', 'semana', 'quantidade']].copy() # Use 'semana' and 'quantity' from df_sem_outliers
y_train_raw = Xy_train_raw.pop('quantidade') # Separate the target variable


# Split the raw data for training the pipeline and evaluation
X_train_pipe, X_val_pipe, y_train_pipe, y_val_pipe = train_test_split(
    Xy_train_raw, y_train_raw, test_size=0.2, random_state=42
)


# =======================================
# 4. Treinamento
# =======================================
print("Treinando pipeline...")
pipeline.fit(X_train_pipe, y_train_pipe)


# =======================================
# 5. Avaliação
# =======================================
print("Avaliando pipeline com dados de validação...")
y_val_pred = pipeline.predict(X_val_pipe)
print("WMAPE validação:", wmape(y_val_pipe, y_val_pred))


# =======================================
# 6. Aplicando no dataset de teste
# =======================================
# Prepare the test dataset with the columns expected by the Preprocessor
# Based on df_teste structure: 'semana', 'pdv', 'produto'
df_teste_processed = df_teste[['semana', 'pdv', 'produto']].copy()

print("Previsões no dataset de teste...")
y_test_pred = pipeline.predict(df_teste_processed)

# Submissão estilo Kaggle
submissao = pd.DataFrame({
    "semana": df_teste_processed["semana"].astype(int), # Include semana and cast to int
    "pdv": df_teste_processed["pdv"].astype(int), # Cast pdv to int
    "produto": df_teste_processed["produto"].astype(int), # Cast produto to int
    "quantidade": y_test_pred.astype(int) # Rename and cast to int
})

# Ensure quantity is non-negative
submissao['quantidade'] = submissao['quantidade'].apply(lambda x: max(0, x))


submissao.to_csv("submissao.csv", index=False)
print("Arquivo submissao.csv gerado!")