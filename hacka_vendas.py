# -*- coding: utf-8 -*-
"""HACKA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BuB-uVeKr_Ph9u4fFDUxc89s6Rial37W

# Colunas do dataset de teste:

 - semana (número inteiro): número da semana (1 a 4 de janeiro/2023)
 - pdv (número inteiro): código do ponto de venda
 - produto (número inteiro): código do SKU
 - quantidade (número inteiro): previsão de vendas
"""

!pip install parquet -q
!pip install pyarrow -q

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import duckdb
import os
import pyarrow.parquet as pq
import pyarrow as pa # Import pyarrow core library

pd.set_option('display.max_columns', 60)
pd.options.mode.chained_assignment = None

import matplotlib.pyplot as plt
# %matplotlib inline

plt.rcParams['font.size'] = 24

from IPython.core.pylabtools import figsize

import seaborn as sns
sns.set(font_scale = 2)

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestRegressor

import warnings
warnings.filterwarnings("ignore")

"""# Modelo deve prever a quantidade com estas colunas no dataset df_teste, gerando a coluna quantidade"""

def missing_values_table(df):
        mis_val = df.isnull().sum()

        mis_val_percent = 100 * df.isnull().sum() / len(df)

        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)

        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values'})

        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)

        print ("Seu dataframe tem " + str(df.shape[1]) + " colunas.\n"
            "Há " + str(mis_val_table_ren_columns.shape[0]) +
              " colunas que possuem valores ausentes.")

        return mis_val_table_ren_columns

def criar_features_para_treinamento(df, min_weeks=8):
    """
    Criar features com tratamento robusto de missing values
    """
    # Ensure 'quantidade' and 'data' columns are present before proceeding
    if 'quantidade' not in df.columns:
        print("Erro: Coluna 'quantidade' não encontrada no DataFrame.")
        return df
    if 'data' not in df.columns:
        print("Erro: Coluna 'data' não encontrada no DataFrame.")
        return df

    # Ensure 'data' is datetime type
    df['data'] = pd.to_datetime(df['data'])

    # Extract year and month from data
    df['ano'] = df['data'].dt.year
    df['mes'] = df['data'].dt.month
    # Keep 'data' for sorting in other functions if needed, or remove here if not.
    # For now, let's keep it as other functions rely on it.

    # Create lags
    for lag in [1, 2, 3, 4, 52]:
        df[f'quantidade_lag_{lag}'] = df.groupby(['pdv', 'produto'])['quantidade'].shift(lag)

    # Moving averages
    for window in [4, 8, 13]:
        df[f'media_movel_{window}'] = df.groupby(['pdv', 'produto'])['quantidade'].transform(
            lambda x: x.rolling(window=window, min_periods=1).mean()
        )

    # Handle missing values in lags and moving averages
    # Let's apply a simple fillna(0) for demonstration after creation
    # A more sophisticated strategy might be needed based on the data
    lag_and_ma_cols = [col for col in df.columns if 'quantidade_lag_' in col or 'media_movel_' in col]
    for col in lag_and_ma_cols:
        df[col] = df[col].fillna(0)


    # Convert relevant columns to float32 to reduce memory usage
    # Convert 'quantidade' and the newly created lag/moving average features
    cols_to_convert_to_float32 = ['quantidade'] + lag_and_ma_cols
    for col in cols_to_convert_to_float32:
        if col in df.columns:
            df[col] = df[col].astype('float32')

    # Convert 'ano', 'mes', 'semana' to smaller integer types if possible
    for col in ['ano', 'mes', 'semana']:
        if col in df.columns:
            try:
                # Use pandas nullable integer type to handle potential NaNs if any crept in
                df[col] = df[col].astype('Int64') # Use Int64 or Int32 depending on range
            except Exception as e:
                print(f"Could not convert column '{col}' to Int64: {e}")
                # Fallback to standard int if no NaNs, otherwise keep original dtype or object
                try:
                     if df[col].isnull().sum() == 0:
                         df[col] = df[col].astype(int)
                except:
                     pass # Keep original dtype if conversion fails


    colunas = df.columns.tolist()
    print("Features:", colunas)
    return df

def criar_features_para_submissao(df_sub, df_treino, ano_previsao=2023):
    """
    Prepara o dataset de submissão (df_sub) adicionando features
    baseadas nos dados históricos (df_treino).
    Assume que df_sub já contém 'pdv', 'produto', e 'semana'.
    """
    df_sub_preparado = df_sub.copy()

    # Adicionar colunas temporais básicas (para 2023)
    df_sub_preparado['ano'] = ano_previsao
    df_sub_preparado['mes'] = 1  # Janeiro para semanas 1-5

    # To get the lag and moving average features for the submission data,
    # we need the *last* calculated features from the historical training data (df_treino).

    # Order historical data by date to easily get the last record for each pdv/produto
    # Ensure df_treino has 'data' column for sorting if it was removed elsewhere, or use another suitable column.
    # Assuming df_treino still has the 'data' column from previous steps.
    if 'data' in df_treino.columns:
        df_treino_sorted = df_treino.sort_values(['pdv', 'produto', 'data'])
    else:
        # If 'data' column is not available, sort by 'ano', 'mes', 'semana'
        df_treino_sorted = df_treino.sort_values(['pdv', 'produto', 'ano', 'mes', 'semana'])


    # Find the last record for each pdv/produto in the historical data (which contains calculated features)
    ultimo_registro_historico = df_treino_sorted.groupby(['pdv', 'produto']).tail(1).copy()

    # Select the feature columns from the last historical record
    # These are the features needed for prediction in the submission data
    features_to_merge = ['pdv', 'produto', 'quantidade_lag_1', 'quantidade_lag_2',
                         'quantidade_lag_3', 'quantidade_lag_4', 'quantidade_lag_52',
                         'media_movel_4', 'media_movel_8', 'media_movel_13']

    # Ensure that the features to merge actually exist in the historical data
    actual_features_to_merge = [col for col in features_to_merge if col in ultimo_registro_historico.columns]

    # Select only the actual features to merge from the last historical record
    ultimo_registro_historico_subset = ultimo_registro_historico[actual_features_to_merge]


    # Merge these historical features onto the submission dataset based on pdv and produto
    # Use left merge to keep all rows in the submission dataset
    df_sub_preparado = df_sub_preparado.merge(
        ultimo_registro_historico_subset,
        on=['pdv', 'produto'],
        how='left'
    )

    # Tratar lag_52 specifically for the correct week of the previous year (in df_treino)
    # We need the average of the same week from the previous year in the historical data
    # This part of the logic might be redundant if 'quantidade_lag_52' is already in ultimo_registro_historico_subset
    # based on how it was calculated in create_training_features.
    # Let's keep it for now but be mindful of potential issues.
    if 'semana' in df_sub_preparado.columns and 'semana' in df_treino.columns:
        semana_ano_anterior_stats = df_treino.groupby(['pdv', 'produto', 'semana'])['quantidade'].mean().reset_index()
        semana_ano_anterior_stats = semana_ano_anterior_stats.rename(columns={'quantidade': 'quantidade_lag_52_calc'})

        # Merge with the submission dataset based on pdv, produto, and semana
        df_sub_preparado = df_sub_preparado.merge(
            semana_ano_anterior_stats,
            on=['pdv', 'produto', 'semana'],
            how='left'
        )

        # If the calculated lag_52 is available, use it, otherwise use the one from ultimo_registro_historico or 0
        df_sub_preparado['quantidade_lag_52'] = df_sub_preparado['quantidade_lag_52_calc'].fillna(
            df_sub_preparado['quantidade_lag_52'].fillna(0) # Fill with merged historical lag_52 if exists, else 0
        )

        # Dropar the temporary column
        df_sub_preparado = df_sub_preparado.drop(columns=['quantidade_lag_52_calc'])
    else:
         # If 'semana' is not available or stats couldn't be calculated, ensure lag_52 is filled with 0
         if 'quantidade_lag_52' not in df_sub_preparado.columns:
              df_sub_preparado['quantidade_lag_52'] = 0
         else:
              df_sub_preparado['quantidade_lag_52'] = df_sub_preparado['quantidade_lag_52'].fillna(0)


    # Preencher NaN remaining in the merged feature columns with 0
    cols_to_fill_zero = [col for col in actual_features_to_merge if col not in ['pdv', 'produto']]
    for col in cols_to_fill_zero:
         if col in df_sub_preparado.columns: # Check if column exists after merge
              df_sub_preparado[col] = df_sub_preparado[col].fillna(0)
         else:
              # If a feature column wasn't in historical data for merging, add it with 0
              df_sub_preparado[col] = 0


    # Define the expected features for the model (must match training features)
    expected_features = ['pdv', 'produto', 'ano', 'mes', 'semana',
                         'quantidade_lag_1', 'quantidade_lag_2', 'quantidade_lag_3',
                         'quantidade_lag_4', 'quantidade_lag_52',
                         'media_movel_4', 'media_movel_8', 'media_movel_13']

    # Ensure the prepared dataframe has all expected features, adding them with 0 if missing
    for feature in expected_features:
        if feature not in df_sub_preparado.columns:
            df_sub_preparado[feature] = 0

    # Select and reorder columns to match the expected features
    df_sub_preparado = df_sub_preparado[expected_features]

    print("\nColumns of the prepared submission dataset:\n", df_sub_preparado.columns.tolist())
    print("\nShape of the prepared submission dataset:", df_sub_preparado.shape)


    return df_sub_preparado

def remove_outliers(df):
    print("Antes:", df.shape)
    first_quartile = df['quantidade'].describe()['25%']
    third_quartile = df['quantidade'].describe()['75%']

    iqr = third_quartile - first_quartile

    df_sem_outliers= df[(df['quantidade'] > (first_quartile - 1.5 * iqr)) &
            (df['quantidade'] < (third_quartile + 1.5 * iqr))].copy()

    # Valores negativos em quantidade são removidos
    df_sem_outliers = df_sem_outliers[df_sem_outliers['quantidade'] >= 0]
    print("Depois:", df_sem_outliers.shape)
    return df_sem_outliers

def criar_dataset_teste_submissao(df_treino):
    """
    Criar dataset de teste para submissão com semanas 1 a 5 de janeiro
    Retorna 5 linhas com pdv e produto diferentes.
    """
    # Identificar combinações únicas de pdv e produto
    combinacoes_unicas = df_treino[['pdv', 'produto']].drop_duplicates()

    # Selecionar as primeiras 5 combinações únicas
    combinacoes_selecionadas = combinacoes_unicas.head(5)

    semanas_janeiro = [1, 2, 3, 4, 5]

    dados_teste = []
    # Para cada uma das 5 combinações selecionadas, criar uma linha com uma semana diferente de 1 a 5
    for i, combo in enumerate(combinacoes_selecionadas.iterrows()):
        pdv_id = combo[1]['pdv'] # Access the row data using index 1
        produto_id = combo[1]['produto'] # Access the row data using index 1

        dados_teste.append({
            'pdv': pdv_id,
            'produto': produto_id,
            'semana': semanas_janeiro[i] # Use the enumerate index 'i' to assign weeks 1-5 sequentially
        })


    # Criar DataFrame
    df_teste = pd.DataFrame(dados_teste)

    # Garantir que as colunas são do tipo int
    df_teste['semana'] = df_teste['semana'].astype(int)

    return df_teste

def treinar_e_avaliar_modelo(df_treino):
    """
    Treinar modelo com dados de treino
    """
    features = ['pdv', 'produto', 'ano', 'mes', 'semana',  'quantidade',
                'quantidade_lag_1', 'quantidade_lag_2', 'quantidade_lag_3',
                'quantidade_lag_4', 'quantidade_lag_52',
                'media_movel_4', 'media_movel_8', 'media_movel_13']

    # Separate features (X) and target (y)
    # Exclude 'data' as it's not needed for the model and 'quantidade' as it's the target
    df_treino = df_treino[features]
    X = df_treino.drop(['quantidade'], axis=1)
    y = df_treino['quantidade']

    # Split the data into training and testing sets
    # Using a test size of 30% and random_state for reproducibility
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    print("Shape of X_train:", X_train.shape)
    print("Shape of X_test:", X_test.shape)
    print("Shape of y_train:", y_train.shape)
    print("Shape of y_test:", y_test.shape)

    print("Treinando modelo")
    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    model.fit(X_train, y_train)
    print("Modelo treinado!")
    y_pred = model.predict(X_test)
    wmape_score = wmape(y_test, y_pred)
    print(f"WMAPE: {wmape_score:.4f} ({wmape_score*100:.2f}%)")
    return model

# Função para calcular WMAPE
def wmape(y_true, y_pred):
    """
    Weighted Mean Absolute Percentage Error
    WMAPE = Σ|y_true - y_pred| / Σ|y_true|
    """
    print('Aplicando WMAPE')
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))

path='/content/drive/MyDrive/HACKA/'

"""# Ler os arquivos parquet com  pyarrow
- cada parte do arquivo parquet foi lida individualmente
- depois foram concatenadas
- a quantidade de linhas e colunas ficou igual a leitura com duckdb
"""

# Get the directory of the current file
directory = os.path.dirname(path)

# List all parquet files in the directory that are part of the dataset
all_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.startswith('part-') and f.endswith('.parquet')]

#print("Arquivos Parquet encontrados:")
#for file in all_files:
    #print(file)
print(f'Lendo e concatenando registros...')

tables = []
for file in all_files:
    try:
        # Read each parquet file into a PyArrow table
        table = pq.read_table(file)
        tables.append(table)
        #print("\nTables Pyarrow:\n", tables)
    except Exception as e:
        print(f"Error reading file {file}: {e}")

if tables:
    # Concatenate the PyArrow tables
    # Use the promote parameter to handle schema differences by promoting to a common type
    combined_table = pa.concat_tables(tables, promote=True)

    # Convert the combined PyArrow Table to a Pandas DataFrame
    df_pyarrow_combined = combined_table.to_pandas()

    print("\nDataset pandas lido e concatenado pelo PyArrow (todas as colunas):")
    print("Colunas:", df_pyarrow_combined.columns.tolist())
    print(df_pyarrow_combined.shape)
else:
    print("No tables were read.")

df_pyarrow_combined.columns

df_pyarrow_combined.head()

"""# Leitura dos arquivos parquet com duckdb
- esta leitura foi descontinuada pois o pyarrow ja fez isso.

# Coluna e tipos esperados no dataset de teste e submissão
- coluna semana no ano (1 a 52)
- pdv (número inteiro): código do ponto de venda
- produto (número inteiro): código do SKU
- quantidade (número inteiro): previsão de vendas
"""

df = df_pyarrow_combined.copy()

"""# Estatistica Descritiva"""

df.describe().T

"""# Tipo de cada coluna"""

df.info()

"""# Percentual de dados ausentes por coluna"""

missing_values_table(df)

"""# Colunas pdv e produto apresentam 99% de dados ausentes.
# Serao substituidas por
- internal_store_id
- internal_product_id
"""

colunas = ['internal_store_id',  'pdv',  'internal_product_id', 'produto', 'transaction_date','quantity']

missing_values_table(df[colunas])

"""# Ano  remover as linhas ausentes em quantity as linhas de outras colunas também foram removidas."""

df = df.dropna(subset=["quantity"]).copy()
missing_values_table(df[colunas])

df.columns

df2 = df[['internal_store_id', 'internal_product_id', 'transaction_date', 'quantity']].copy()

"""# Coluna são renomeadas"""

df2.rename(columns={'internal_store_id': 'pdv', 'internal_product_id': 'produto', 'quantity': 'quantidade', 'transaction_date': 'data'}, inplace=True)

df2.isna().sum()

"""# Extrair o numero da semana no ano a partir da data de transacao"""

df2['data'] = pd.to_datetime(df2['data'])
df2['semana'] = df2['data'].dt.isocalendar().week

df2.columns

df2

missing_values_table(df2)

df2.info()

df2

df2.nunique()

"""# Coluna quantidade tem valores negativos
# Outlier certamente.
"""

df2.describe()

df_sem_outliers = remove_outliers(df2)

"""# Extrair apenas 10% dos dados"""

df_100 = df_sem_outliers.sample(frac=1, random_state=42)
df_100.shape

df_100.head()

df_treino = criar_features_para_treinamento(df_100)

df_treino.isna().sum()

df_treino.info()

df_sub = criar_dataset_teste_submissao(df_treino)

df_sub

df_sub_preparado = criar_features_para_submissao(df_sub, df_treino)

model = treinar_e_avaliar_modelo(df_treino)

df_sub['quantidade'] = model.predict(df_sub_preparado)
df_sub

# Round the predicted quantities to the nearest integer before converting to int type
df_sub['quantidade'] = df_sub['quantidade'].round().astype(int)

df_sub

# Use the df_sub with the rounded integer 'quantidade'
df_envio = df_sub[['pdv', 'produto', 'semana', 'quantidade']].copy()
df_envio['produto'] = df_envio['produto'].astype(int)
df_envio['pdv'] = df_envio['pdv'].astype(int)
df_envio['semana'] = df_envio['semana'].astype(int)
df_envio.columns = ['semana','pdv', 'produto', 'quantidade']

df_envio.to_csv('submission.csv', sep=';', encoding='utf-8', index=False)
display(df_envio)

df_envio.info()

